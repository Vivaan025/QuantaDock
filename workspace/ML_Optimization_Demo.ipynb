{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30e2bf08",
   "metadata": {},
   "source": [
    "# üöÄ ML Model Optimization Suite\n",
    "\n",
    "This notebook demonstrates comprehensive optimization of different ML model types:\n",
    "- **LLMs** (Large Language Models)\n",
    "- **Stable Diffusion** (Image Generation)\n",
    "- **Text Encoders** (Embeddings & NLP)\n",
    "- **Audio Models** (Sound-to-Vector)\n",
    "\n",
    "Each optimization includes hardware-specific tuning and detailed performance analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef80a46",
   "metadata": {},
   "source": [
    "## üîç Step 1: System Hardware Analysis\n",
    "\n",
    "First, let's analyze your hardware specifications to determine optimal optimization strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1b3b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/workspace/scripts')\n",
    "sys.path.append('/workspace/configs')\n",
    "\n",
    "import json\n",
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "# Run system analysis\n",
    "print(\"üîç Analyzing system hardware...\")\n",
    "!python /workspace/scripts/system_info.py\n",
    "\n",
    "# Load and display key specs\n",
    "with open('/workspace/system_specs.json', 'r') as f:\n",
    "    specs = json.load(f)\n",
    "\n",
    "print(\"\\nüìä Key Hardware Specifications:\")\n",
    "print(f\"CPU Cores: {specs['cpu']['cores_logical']}\")\n",
    "print(f\"RAM: {specs['memory']['total_gb']} GB\")\n",
    "print(f\"GPU Count: {len(specs['gpu'])}\")\n",
    "if specs['gpu']:\n",
    "    for i, gpu in enumerate(specs['gpu']):\n",
    "        print(f\"GPU {i}: {gpu['name']} ({gpu['total_memory_gb']} GB VRAM)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83e0dad",
   "metadata": {},
   "source": [
    "## ü§ñ Step 2: LLM Optimization\n",
    "\n",
    "Optimize Large Language Models with quantization and LoRA techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3ad2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ü§ñ Starting LLM Optimization...\")\n",
    "print(\"Using INT8 quantization and LoRA for efficient fine-tuning\")\n",
    "\n",
    "# Run LLM optimization\n",
    "!python /workspace/scripts/optimize_llm.py --model microsoft/DialoGPT-small --quantization int8 --use-lora\n",
    "\n",
    "# Load and display results\n",
    "results_file = Path('/workspace/results/llm_optimization_results_int8.json')\n",
    "if results_file.exists():\n",
    "    with open(results_file, 'r') as f:\n",
    "        llm_results = json.load(f)\n",
    "    \n",
    "    print(\"\\nüìä LLM Optimization Results:\")\n",
    "    if 'benchmark' in llm_results:\n",
    "        benchmark = llm_results['benchmark']\n",
    "        print(f\"Tokens per second: {benchmark.get('tokens_per_second', 'N/A')}\")\n",
    "        print(f\"Average time per run: {benchmark.get('avg_time_per_run', 'N/A'):.3f}s\")\n",
    "        print(f\"Peak memory usage: {benchmark.get('peak_memory_gb', 'N/A'):.2f} GB\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è LLM optimization results not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225d2e75",
   "metadata": {},
   "source": [
    "## üé® Step 3: Stable Diffusion Optimization\n",
    "\n",
    "Optimize Stable Diffusion for faster image generation with memory efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447de340",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üé® Starting Stable Diffusion Optimization...\")\n",
    "print(\"Using FP16 precision with memory-efficient attention\")\n",
    "\n",
    "# Run Stable Diffusion optimization\n",
    "!python /workspace/scripts/optimize_stable_diffusion.py --model runwayml/stable-diffusion-v1-5 --precision fp16\n",
    "\n",
    "# Load and display results\n",
    "results_file = Path('/workspace/results/sd_optimization_results_fp16.json')\n",
    "if results_file.exists():\n",
    "    with open(results_file, 'r') as f:\n",
    "        sd_results = json.load(f)\n",
    "    \n",
    "    print(\"\\nüìä Stable Diffusion Optimization Results:\")\n",
    "    if 'single_image_benchmark' in sd_results:\n",
    "        benchmark = sd_results['single_image_benchmark']\n",
    "        print(f\"Average time per image: {benchmark.get('avg_time_per_image', 'N/A'):.2f}s\")\n",
    "        print(f\"Peak memory usage: {benchmark.get('peak_memory_gb', 'N/A'):.2f} GB\")\n",
    "        print(f\"Images generated: {benchmark.get('images_generated', 'N/A')}\")\n",
    "    \n",
    "    if 'batch_benchmark' in sd_results:\n",
    "        batch = sd_results['batch_benchmark']\n",
    "        print(f\"Batch throughput: {batch.get('throughput_images_per_second', 'N/A'):.2f} images/sec\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Stable Diffusion optimization results not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933306cb",
   "metadata": {},
   "source": [
    "## üìù Step 4: Text Encoder Optimization\n",
    "\n",
    "Optimize text encoders for efficient embedding generation and similarity search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7844ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìù Starting Text Encoder Optimization...\")\n",
    "print(\"Optimizing sentence transformers for embedding generation\")\n",
    "\n",
    "# Run text encoder optimization\n",
    "!python /workspace/scripts/optimize_text_encoder.py --model sentence-transformers/all-MiniLM-L6-v2\n",
    "\n",
    "# Load and display results\n",
    "results_file = Path('/workspace/results/text_encoder_optimization_results_sentence_transformer.json')\n",
    "if results_file.exists():\n",
    "    with open(results_file, 'r') as f:\n",
    "        text_results = json.load(f)\n",
    "    \n",
    "    print(\"\\nüìä Text Encoder Optimization Results:\")\n",
    "    if 'encoding_benchmark' in text_results:\n",
    "        # Show best performing batch size\n",
    "        benchmark = text_results['encoding_benchmark']\n",
    "        best_batch = max(benchmark.items(), key=lambda x: x[1]['texts_per_second'])\n",
    "        batch_name, batch_results = best_batch\n",
    "        print(f\"Best batch size: {batch_name}\")\n",
    "        print(f\"Texts per second: {batch_results['texts_per_second']:.2f}\")\n",
    "        print(f\"Peak memory: {batch_results['peak_memory_gb']:.2f} GB\")\n",
    "    \n",
    "    if 'similarity_benchmark' in text_results:\n",
    "        sim = text_results['similarity_benchmark']\n",
    "        print(f\"Similarity search: {sim.get('queries_per_second', 'N/A'):.2f} queries/sec\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Text encoder optimization results not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58eca96",
   "metadata": {},
   "source": [
    "## üîä Step 5: Audio Model Optimization\n",
    "\n",
    "Optimize audio models for sound-to-vector conversion with preprocessing enhancements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3106a6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîä Starting Audio Model Optimization...\")\n",
    "print(\"Optimizing Wav2Vec2 for audio embedding generation\")\n",
    "\n",
    "# Run audio model optimization\n",
    "!python /workspace/scripts/optimize_sound_to_vec.py --model facebook/wav2vec2-base-960h --enable-preprocessing\n",
    "\n",
    "# Load and display results\n",
    "results_file = Path('/workspace/results/audio_optimization_results_fp32.json')\n",
    "if results_file.exists():\n",
    "    with open(results_file, 'r') as f:\n",
    "        audio_results = json.load(f)\n",
    "    \n",
    "    print(\"\\nüìä Audio Model Optimization Results:\")\n",
    "    if 'encoding_benchmark' in audio_results:\n",
    "        # Show best performing batch size\n",
    "        benchmark = audio_results['encoding_benchmark']\n",
    "        if benchmark:\n",
    "            best_batch = max(benchmark.items(), key=lambda x: x[1]['audio_clips_per_second'])\n",
    "            batch_name, batch_results = best_batch\n",
    "            print(f\"Best batch size: {batch_name}\")\n",
    "            print(f\"Audio clips per second: {batch_results['audio_clips_per_second']:.2f}\")\n",
    "            print(f\"Peak memory: {batch_results['peak_memory_gb']:.2f} GB\")\n",
    "    \n",
    "    if 'preprocessing_optimization' in audio_results:\n",
    "        preproc = audio_results['preprocessing_optimization']\n",
    "        best_preproc = max(preproc.items(), key=lambda x: x[1]['samples_per_second'])\n",
    "        preproc_name, preproc_results = best_preproc\n",
    "        print(f\"Best preprocessing: {preproc_name}\")\n",
    "        print(f\"Samples per second: {preproc_results['samples_per_second']:.2f}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Audio optimization results not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e948fa1",
   "metadata": {},
   "source": [
    "## üìä Step 6: Comprehensive Performance Analysis\n",
    "\n",
    "Generate and display a comprehensive performance report with optimization recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2de5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìä Generating Comprehensive Performance Report...\")\n",
    "\n",
    "# Run master optimizer to generate comprehensive report\n",
    "!python /workspace/scripts/master_optimizer.py\n",
    "\n",
    "# Load comprehensive report\n",
    "report_file = Path('/workspace/results/comprehensive_optimization_report.json')\n",
    "if report_file.exists():\n",
    "    with open(report_file, 'r') as f:\n",
    "        report = json.load(f)\n",
    "    \n",
    "    print(\"\\nüéØ COMPREHENSIVE PERFORMANCE REPORT\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # System Summary\n",
    "    if 'system_summary' in report:\n",
    "        sys_summary = report['system_summary']\n",
    "        print(f\"\\nüñ•Ô∏è System Configuration:\")\n",
    "        print(f\"  CPU Cores: {sys_summary.get('cpu_cores', 'Unknown')}\")\n",
    "        print(f\"  Memory: {sys_summary.get('memory_gb', 'Unknown')} GB\")\n",
    "        print(f\"  GPU Count: {sys_summary.get('gpu_count', 'Unknown')}\")\n",
    "        print(f\"  Total GPU Memory: {sys_summary.get('gpu_memory_gb', 'Unknown')} GB\")\n",
    "    \n",
    "    # Performance Metrics\n",
    "    if 'performance_metrics' in report:\n",
    "        print(f\"\\n‚ö° Performance Metrics:\")\n",
    "        for model_type, metrics in report['performance_metrics'].items():\n",
    "            print(f\"  {model_type.replace('_', ' ').title()}:\")\n",
    "            print(f\"    Processing Speed: {metrics.get('processing_speed', 'N/A')}\")\n",
    "            print(f\"    Memory Usage: {metrics.get('memory_usage_gb', 'N/A')} GB\")\n",
    "            print(f\"    Avg Processing Time: {metrics.get('avg_processing_time', 'N/A')}s\")\n",
    "    \n",
    "    # Recommendations\n",
    "    if 'recommendations' in report and report['recommendations']:\n",
    "        print(f\"\\nüí° Optimization Recommendations:\")\n",
    "        for i, rec in enumerate(report['recommendations'], 1):\n",
    "            print(f\"  {i}. {rec}\")\n",
    "    else:\n",
    "        print(f\"\\n‚úÖ Your system is well-optimized for the tested models!\")\n",
    "    \n",
    "    print(f\"\\nüìÅ Detailed results saved to: /workspace/results/\")\n",
    "    print(f\"üìä Full report: comprehensive_optimization_report.json\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Comprehensive report not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3c67b9",
   "metadata": {},
   "source": [
    "## üîç Step 7: View Generated Images (Stable Diffusion)\n",
    "\n",
    "Display images generated during the Stable Diffusion optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322ba87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "# Look for generated images\n",
    "results_dir = Path('/workspace/results')\n",
    "image_files = list(results_dir.glob('generated_image_*.png'))\n",
    "\n",
    "if image_files:\n",
    "    print(f\"üé® Displaying {len(image_files)} generated images:\")\n",
    "    \n",
    "    # Create subplot for images\n",
    "    fig, axes = plt.subplots(1, min(len(image_files), 4), figsize=(16, 4))\n",
    "    if len(image_files) == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for i, img_file in enumerate(image_files[:4]):\n",
    "        img = Image.open(img_file)\n",
    "        if i < len(axes):\n",
    "            axes[i].imshow(img)\n",
    "            axes[i].axis('off')\n",
    "            axes[i].set_title(f'Image {i+1}')\n",
    "    \n",
    "    # Hide unused subplots\n",
    "    for i in range(len(image_files), len(axes)):\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No generated images found. Run Stable Diffusion optimization first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09bc6d49",
   "metadata": {},
   "source": [
    "## üìà Step 8: Performance Visualization\n",
    "\n",
    "Create visualizations of the optimization results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fcba7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Collect performance data from all optimizations\n",
    "performance_data = {}\n",
    "memory_data = {}\n",
    "\n",
    "# Try to load all result files\n",
    "result_files = {\n",
    "    'LLM (INT8)': '/workspace/results/llm_optimization_results_int8.json',\n",
    "    'Stable Diffusion (FP16)': '/workspace/results/sd_optimization_results_fp16.json',\n",
    "    'Text Encoder': '/workspace/results/text_encoder_optimization_results_sentence_transformer.json',\n",
    "    'Audio Model': '/workspace/results/audio_optimization_results_fp32.json'\n",
    "}\n",
    "\n",
    "for model_name, file_path in result_files.items():\n",
    "    if Path(file_path).exists():\n",
    "        with open(file_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        # Extract performance metrics\n",
    "        if model_name.startswith('LLM') and 'benchmark' in data:\n",
    "            performance_data[model_name] = data['benchmark'].get('tokens_per_second', 0)\n",
    "            memory_data[model_name] = data['benchmark'].get('peak_memory_gb', 0)\n",
    "        elif model_name.startswith('Stable Diffusion') and 'single_image_benchmark' in data:\n",
    "            # Convert to images per second\n",
    "            avg_time = data['single_image_benchmark'].get('avg_time_per_image', 1)\n",
    "            performance_data[model_name] = 1 / avg_time if avg_time > 0 else 0\n",
    "            memory_data[model_name] = data['single_image_benchmark'].get('peak_memory_gb', 0)\n",
    "        elif model_name.startswith('Text Encoder') and 'encoding_benchmark' in data:\n",
    "            # Get best batch performance\n",
    "            benchmark = data['encoding_benchmark']\n",
    "            if benchmark:\n",
    "                best_perf = max(batch['texts_per_second'] for batch in benchmark.values())\n",
    "                performance_data[model_name] = best_perf\n",
    "                best_batch = max(benchmark.items(), key=lambda x: x[1]['texts_per_second'])[1]\n",
    "                memory_data[model_name] = best_batch.get('peak_memory_gb', 0)\n",
    "        elif model_name.startswith('Audio') and 'encoding_benchmark' in data:\n",
    "            # Get best batch performance\n",
    "            benchmark = data['encoding_benchmark']\n",
    "            if benchmark:\n",
    "                best_perf = max(batch['audio_clips_per_second'] for batch in benchmark.values())\n",
    "                performance_data[model_name] = best_perf\n",
    "                best_batch = max(benchmark.items(), key=lambda x: x[1]['audio_clips_per_second'])[1]\n",
    "                memory_data[model_name] = best_batch.get('peak_memory_gb', 0)\n",
    "\n",
    "# Create visualizations\n",
    "if performance_data:\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Performance chart\n",
    "    models = list(performance_data.keys())\n",
    "    performance = list(performance_data.values())\n",
    "    \n",
    "    bars1 = ax1.bar(models, performance, color=['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4'])\n",
    "    ax1.set_title('Model Performance (Items/Second)', fontsize=14, fontweight='bold')\n",
    "    ax1.set_ylabel('Items per Second')\n",
    "    ax1.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, value in zip(bars1, performance):\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(performance)*0.01,\n",
    "                f'{value:.1f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # Memory usage chart\n",
    "    memory = [memory_data.get(model, 0) for model in models]\n",
    "    bars2 = ax2.bar(models, memory, color=['#FFB6C1', '#B6E5D8', '#B6D4F1', '#D4F1C9'])\n",
    "    ax2.set_title('Peak Memory Usage (GB)', fontsize=14, fontweight='bold')\n",
    "    ax2.set_ylabel('Memory (GB)')\n",
    "    ax2.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, value in zip(bars2, memory):\n",
    "        if value > 0:\n",
    "            ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(memory)*0.01,\n",
    "                    f'{value:.1f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"üìä Performance Summary:\")\n",
    "    for model, perf in performance_data.items():\n",
    "        mem = memory_data.get(model, 0)\n",
    "        print(f\"  {model}: {perf:.1f} items/sec, {mem:.1f} GB memory\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No performance data available. Run optimizations first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7a778d",
   "metadata": {},
   "source": [
    "## üéØ Summary and Next Steps\n",
    "\n",
    "This notebook demonstrated comprehensive ML model optimization with hardware-specific tuning. Here's what we accomplished:\n",
    "\n",
    "### ‚úÖ Completed Optimizations\n",
    "1. **System Analysis**: Hardware specifications and capabilities\n",
    "2. **LLM Optimization**: Quantization and LoRA for efficient inference\n",
    "3. **Stable Diffusion**: Memory-efficient image generation\n",
    "4. **Text Encoders**: Batch optimization and embedding compression\n",
    "5. **Audio Models**: Sound processing and feature extraction\n",
    "\n",
    "### üìä Key Metrics Tracked\n",
    "- **Throughput**: Items processed per second\n",
    "- **Memory Usage**: Peak GPU memory consumption\n",
    "- **Latency**: Average processing time per item\n",
    "- **Hardware Utilization**: GPU and memory efficiency\n",
    "\n",
    "### üöÄ Next Steps\n",
    "1. **Production Deployment**: Export optimized models for serving\n",
    "2. **Custom Models**: Adapt scripts for your specific models\n",
    "3. **Advanced Optimization**: TensorRT, ONNX export, multi-GPU\n",
    "4. **Monitoring**: Set up continuous performance monitoring\n",
    "\n",
    "### üìÅ Generated Files\n",
    "- **Results**: `/workspace/results/` - All optimization results\n",
    "- **Models**: `/workspace/models/` - Optimized model exports\n",
    "- **Images**: Generated Stable Diffusion samples\n",
    "- **Reports**: Comprehensive performance analysis\n",
    "\n",
    "The Docker environment provides exact hardware specification matching and comprehensive benchmarking for optimal ML model performance!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
